{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case you haven't installed the API\n",
    "! pip install nytimesarticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kshain/anaconda/envs/AC209/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "from ProgressBar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "api = articleAPI('ef0f07b0a98f450c9a11d3c2f25f4b67') #Kevin\n",
    "#api = articleAPI('9f6355bf925a4af9b5d296791a35863e') #Kevin\n",
    "#api = articleAPI('7b0535e75077457b97eabb75f52e2a5b') #Kevin\n",
    "#api = articleAPI('4303822ccc8249a38913e858ec549574') #Kevin\n",
    "#api = articleAPI('440fbcc705364b82b35b52cd5f4b4979') #Kevin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving NY Times data to file\n",
    "It is essential that we save the news data to file before experimenting with feature extraction and modeling so we don't flood the NY Times servers with requests. This way, we only need to make calls to get the data once. Furthermore, we make the function sleep for 1 second between calls so as to not stress the server.\n",
    "\n",
    "There are some peculiarities about using the NY Times Article Search API that were found in experimentation. The first is that only the first 100 pages from a given search are callable. This means that a particularly general search or a long search window will lead to inaccessible results. Therefore, the function breaks the search window up into single weeks so that there are never over 100 pages of results.\n",
    "\n",
    "After each week is extracted, the data is zipped together and appended to the output csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def downloadToFile(startdate, enddate, filename):\n",
    "    \"\"\"\n",
    "    Makes API calls to extract id, publication date, headline, and lead paragraph from NY Times articles in the date range.\n",
    "    Then, saves the data to a local file in csv format.\n",
    "    startdate: start of date range to extract (yyyymmdd)\n",
    "    enddate: end of date range to extract (yyyymmdd)\n",
    "    filename: csv file to append to\n",
    "    \"\"\"\n",
    "    \n",
    "    startdate = datetime.datetime.strptime(str(startdate), '%Y%m%d')\n",
    "    enddate = datetime.datetime.strptime(str(enddate), '%Y%m%d')\n",
    "\n",
    "    sliceStart = startdate\n",
    "\n",
    "    while (sliceStart<enddate):\n",
    "        leads = []\n",
    "        ids = []\n",
    "        dates = []\n",
    "        headlines = []\n",
    "        \n",
    "        sliceEnd = min(sliceStart + datetime.timedelta(weeks=1), enddate)\n",
    "\n",
    "        sliceStartInt = int(sliceStart.strftime('%Y%m%d'))\n",
    "        sliceEndInt = int(sliceEnd.strftime('%Y%m%d'))\n",
    "        print 'Downloading from {} to {}'.format(sliceStartInt, sliceEndInt)\n",
    "        while True:\n",
    "            try:\n",
    "                numhits = api.search(fl = ['_id'],begin_date = sliceStartInt, end_date=sliceEndInt,fq = {'section_name':'Business'}, page=1)['response']['meta']['hits']\n",
    "                break\n",
    "            except:\n",
    "                print 'JSON error avoided'\n",
    "        pages = int(math.ceil(float(numhits)/10))\n",
    "        time.sleep(1)\n",
    "        pbar2 = ProgressBar(pages)\n",
    "        print '{} pages to download'.format(pages) # Note that you can't download past page number 100\n",
    "        for page in range(1,pages+1):\n",
    "            while True:\n",
    "                try:\n",
    "                    articles = api.search(fl= ['_id','headline','lead_paragraph','pub_date'], begin_date = sliceStartInt, end_date=sliceEndInt,fq = {'section_name':'Business'}, page=page)\n",
    "                    break\n",
    "                except:\n",
    "                    print 'JSON error avoided'\n",
    "            time.sleep(1)\n",
    "            pbar2.increment()\n",
    "            for i in articles['response']['docs']:\n",
    "                if (i['lead_paragraph'] is not None) and (i['headline'] != []):\n",
    "                    headlines.append(i['headline']['main'])\n",
    "                    leads.append(i['lead_paragraph'])\n",
    "                    ids.append(i['_id'])\n",
    "                    dates.append(i['pub_date'])\n",
    "\n",
    "        pbar2.finish()\n",
    "        sliceStart = sliceEnd\n",
    "\n",
    "        zipped = zip(ids, dates, headlines, leads)\n",
    "        if zipped:\n",
    "            with open(filename, \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                for line in zipped: \n",
    "                    writer.writerow([unicode(s).encode(\"utf-8\") for s in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from 19980212 to 19980219\n",
      "28 pages to download\n",
      "Complete! Total Elapsed time: 39.0 seconds                        \n",
      "Downloading from 19980219 to 19980226\n",
      "31 pages to download\n",
      "Loading: |>-------------------|  9%  Elapsed time: 4.1 seconds"
     ]
    }
   ],
   "source": [
    "downloadToFile(19980212, 19981231, '1998_Output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:AC209]",
   "language": "python",
   "name": "conda-env-AC209-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
