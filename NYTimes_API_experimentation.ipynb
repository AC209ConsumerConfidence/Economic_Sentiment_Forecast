{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case you haven't installed the API\n",
    "! pip install nytimesarticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "from ProgressBar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api = articleAPI('ef0f07b0a98f450c9a11d3c2f25f4b67') #Kevin\n",
    "#api = articleAPI('9f6355bf925a4af9b5d296791a35863e') #Kevin\n",
    "#api = articleAPI('7b0535e75077457b97eabb75f52e2a5b') #Kevin\n",
    "#api = articleAPI('4303822ccc8249a38913e858ec549574') #Kevin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving NY Times data to file\n",
    "It is essential that we save the news data to file before experimenting with feature extraction and modeling so we don't flood the NY Times servers with requests. This way, we only need to make calls to get the data once. Furthermore, we make the function sleep for 1 second between calls so as to not stress the server.\n",
    "\n",
    "There are some peculiarities about using the NY Times Article Search API that were found in experimentation. The first is that only the first 100 pages from a given search are callable. This means that a particularly general search or a long search window will lead to inaccessible results. Therefore, the function breaks the search window up into single weeks so that there are never over 100 pages of results.\n",
    "\n",
    "After each week is extracted, the data is zipped together and appended to the output csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloadToFile(startdate, enddate, filename):\n",
    "    \"\"\"\n",
    "    Makes API calls to extract id, publication date, headline, and lead paragraph from NY Times articles in the date range.\n",
    "    Then, saves the data to a local file in csv format.\n",
    "    startdate: start of date range to extract (yyyymmdd)\n",
    "    enddate: end of date range to extract (yyyymmdd)\n",
    "    filename: csv file to append to\n",
    "    \"\"\"\n",
    "    \n",
    "    startdate = datetime.datetime.strptime(str(startdate), '%Y%m%d')\n",
    "    enddate = datetime.datetime.strptime(str(enddate), '%Y%m%d')\n",
    "\n",
    "    sliceStart = startdate\n",
    "\n",
    "    while (sliceStart<enddate):\n",
    "        leads = []\n",
    "        ids = []\n",
    "        dates = []\n",
    "        headlines = []\n",
    "        \n",
    "        sliceEnd = min(sliceStart + datetime.timedelta(weeks=1), enddate)\n",
    "\n",
    "        sliceStartInt = int(sliceStart.strftime('%Y%m%d'))\n",
    "        sliceEndInt = int(sliceEnd.strftime('%Y%m%d'))\n",
    "        print 'Downloading from {} to {}'.format(sliceStartInt, sliceEndInt)\n",
    "        numhits = api.search(fl = ['_id'],begin_date = sliceStartInt, end_date=sliceEndInt,fq = {'section_name':'Business'}, page=1)['response']['meta']['hits']\n",
    "        pages = int(math.ceil(float(numhits)/10))\n",
    "        time.sleep(1)\n",
    "        pbar2 = ProgressBar(pages)\n",
    "        print '{} pages to download'.format(pages) # Note that you can't download past page number 100\n",
    "        for page in range(1,pages+1):\n",
    "            articles = api.search(fl= ['_id','headline','lead_paragraph','pub_date'], begin_date = sliceStartInt, end_date=sliceEndInt,fq = {'section_name':'Business'}, page=page)\n",
    "            time.sleep(1)\n",
    "            pbar2.increment()\n",
    "            for i in articles['response']['docs']:\n",
    "                if i['lead_paragraph'] is not None:\n",
    "                    headlines.append(i['headline']['main'])\n",
    "                    leads.append(i['lead_paragraph'])\n",
    "                    ids.append(i['_id'])\n",
    "                    dates.append(i['pub_date'])\n",
    "\n",
    "        pbar2.finish()\n",
    "        sliceStart = sliceEnd\n",
    "\n",
    "        zipped = zip(ids, dates, headlines, leads)\n",
    "        if zipped:\n",
    "            with open(filename, \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                for line in zipped: \n",
    "                    writer.writerow([unicode(s).encode(\"utf-8\") for s in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from 20020101 to 20020108\n",
      "32 pages to download\n",
      "Complete! Total Elapsed time: 46.8 seconds                        \n",
      "Downloading from 20020108 to 20020115\n",
      "40 pages to download\n",
      "Complete! Total Elapsed time: 57.6 seconds                        \n",
      "Downloading from 20020115 to 20020122\n",
      "38 pages to download\n",
      "Complete! Total Elapsed time: 55.1 seconds                        \n",
      "Downloading from 20020122 to 20020129\n",
      "40 pages to download\n",
      "Complete! Total Elapsed time: 58.4 seconds                        \n",
      "Downloading from 20020129 to 20020205\n",
      "40 pages to download\n",
      "Complete! Total Elapsed time: 59.0 seconds                        \n",
      "Downloading from 20020205 to 20020212\n",
      "42 pages to download\n",
      "Complete! Total Elapsed time: 62.5 seconds                        \n",
      "Downloading from 20020212 to 20020219\n",
      "36 pages to download\n",
      "Complete! Total Elapsed time: 54.4 seconds                        \n",
      "Downloading from 20020219 to 20020226\n",
      "36 pages to download\n",
      "Complete! Total Elapsed time: 50.6 seconds                        \n",
      "Downloading from 20020226 to 20020305\n",
      "40 pages to download\n",
      "Complete! Total Elapsed time: 59.4 seconds                        \n",
      "Downloading from 20020305 to 20020312\n",
      "40 pages to download\n",
      "Complete! Total Elapsed time: 59.1 seconds                        \n",
      "Downloading from 20020312 to 20020319\n",
      "38 pages to download\n",
      "Complete! Total Elapsed time: 53.4 seconds                        \n",
      "Downloading from 20020319 to 20020326\n",
      "38 pages to download\n",
      "Complete! Total Elapsed time: 55.1 seconds                        \n",
      "Downloading from 20020326 to 20020402\n",
      "37 pages to download\n",
      "Complete! Total Elapsed time: 55.2 seconds                        \n",
      "Downloading from 20020402 to 20020409\n",
      "41 pages to download\n",
      "Complete! Total Elapsed time: 61.1 seconds                        \n",
      "Downloading from 20020409 to 20020416\n",
      "41 pages to download\n",
      "Complete! Total Elapsed time: 59.3 seconds                        \n",
      "Downloading from 20020416 to 20020423\n",
      "40 pages to download\n",
      "Complete! Total Elapsed time: 58.4 seconds                        \n",
      "Downloading from 20020423 to 20020430\n",
      "40 pages to download\n",
      "Complete! Total Elapsed time: 62.5 seconds                        \n",
      "Downloading from 20020430 to 20020507\n",
      "40 pages to download\n",
      "Complete! Total Elapsed time: 62.9 seconds                        \n",
      "Downloading from 20020507 to 20020514\n",
      "39 pages to download\n",
      "Complete! Total Elapsed time: 62.1 seconds                        \n",
      "Downloading from 20020514 to 20020521\n",
      "38 pages to download\n",
      "Complete! Total Elapsed time: 53.9 seconds                        \n",
      "Downloading from 20020521 to 20020528\n",
      "35 pages to download\n",
      "Complete! Total Elapsed time: 49.9 seconds                        \n",
      "Downloading from 20020528 to 20020604\n",
      "37 pages to download\n",
      "Complete! Total Elapsed time: 53.8 seconds                        \n",
      "Downloading from 20020604 to 20020611\n",
      "37 pages to download\n",
      "Complete! Total Elapsed time: 55.7 seconds                        \n",
      "Downloading from 20020611 to 20020618\n",
      "39 pages to download\n",
      "Complete! Total Elapsed time: 61.7 seconds                        \n",
      "Downloading from 20020618 to 20020625\n",
      "38 pages to download\n",
      "Complete! Total Elapsed time: 56.3 seconds                        \n",
      "Downloading from 20020625 to 20020702\n",
      "38 pages to download\n",
      "Loading: |>>>>>>--------------|  34%  Elapsed time: 18.7 seconds"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f72df753deb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdownloadToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20020101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20021231\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2002_Output.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-1b33f49e063e>\u001b[0m in \u001b[0;36mdownloadToFile\u001b[0;34m(startdate, enddate, filename)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mpbar2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lead_paragraph'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mheadlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'headline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'response'"
     ]
    }
   ],
   "source": [
    "downloadToFile(20020101, 20021231, '2002_Output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:AC209]",
   "language": "python",
   "name": "conda-env-AC209-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
