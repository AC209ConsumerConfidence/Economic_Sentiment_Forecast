{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kshain/anaconda/envs/AC209/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "from ProgressBar import ProgressBar\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw output from the NY Times API is stored as separate csv files for each year. We currently have all the data from 1997 onwards, but will just show a subset as an example here. The data includes the date of publication, article id, headline, and lead paragraph. We then stem the lead paragragh to increase the accuracy of the bag of words by eliminating meaningless variations of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_list = []\n",
    "for year in range(2000,2005):\n",
    "    data = pd.read_csv('{}_Output.csv'.format(year), header=None)\n",
    "    all_data_list.append(data) # list of dataframes\n",
    "data = pd.concat(all_data_list, axis=0)\n",
    "data.columns = ['id','date','headline', 'lead']\n",
    "data['yearmonth'] = map(lambda x: x[:7], data.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStems(lead):\n",
    "    stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "    tokens = nltk.word_tokenize(''.join(ch for ch in lead if ch not in set(string.punctuation)))\n",
    "    return map(lambda x: stemmer.stem(x.decode('utf-8')), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def joinstems(stemlist):\n",
    "    return ' '.join(stem for stem in stemlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stems = map(getStems, data.lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['leadstems'] = map(joinstems, stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>lead</th>\n",
       "      <th>yearmonth</th>\n",
       "      <th>leadstems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4fd233bf8eb7c8105d7c430b</td>\n",
       "      <td>2000-01-08T00:00:00Z</td>\n",
       "      <td>THE MARKETS: COMMODITIES</td>\n",
       "      <td>CRUDE OIL FALLS. Crude oil declined more than ...</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>crude oil fall crude oil declin more than 2 pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4fd237718eb7c8105d7c9aa8</td>\n",
       "      <td>2000-01-08T00:00:00Z</td>\n",
       "      <td>DIMINISHED BRAZILIAN INFLATION</td>\n",
       "      <td>Concern over Brazilian price increases eased a...</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>concern over brazilian price increas eas after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4fd21c448eb7c8105d79c973</td>\n",
       "      <td>2000-01-08T00:00:00Z</td>\n",
       "      <td>Merrill Lynch Reimburses Client For Loss Linke...</td>\n",
       "      <td>Merrill Lynch &amp; Company said today that it rei...</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>merril lynch compani said today that it reimbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4fd1f22e8eb7c8105d7496d1</td>\n",
       "      <td>2000-01-08T00:00:00Z</td>\n",
       "      <td>NEW UNICOM-PECO MERGER TERMS INCLUDE STOCK BUY...</td>\n",
       "      <td>The Unicom Corporation and the Peco Energy Com...</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>the unicom corpor and the peco energi compani ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fd203328eb7c8105d768db0</td>\n",
       "      <td>2000-01-08T00:00:00Z</td>\n",
       "      <td>SYSCO AGREES TO BUY FRUIT AND VEGETABLE DISTRI...</td>\n",
       "      <td>The Sysco Corporation, North America's largest...</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>the sysco corpor north america largest food se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                  date  \\\n",
       "0  4fd233bf8eb7c8105d7c430b  2000-01-08T00:00:00Z   \n",
       "1  4fd237718eb7c8105d7c9aa8  2000-01-08T00:00:00Z   \n",
       "2  4fd21c448eb7c8105d79c973  2000-01-08T00:00:00Z   \n",
       "3  4fd1f22e8eb7c8105d7496d1  2000-01-08T00:00:00Z   \n",
       "4  4fd203328eb7c8105d768db0  2000-01-08T00:00:00Z   \n",
       "\n",
       "                                            headline  \\\n",
       "0                           THE MARKETS: COMMODITIES   \n",
       "1                     DIMINISHED BRAZILIAN INFLATION   \n",
       "2  Merrill Lynch Reimburses Client For Loss Linke...   \n",
       "3  NEW UNICOM-PECO MERGER TERMS INCLUDE STOCK BUY...   \n",
       "4  SYSCO AGREES TO BUY FRUIT AND VEGETABLE DISTRI...   \n",
       "\n",
       "                                                lead yearmonth  \\\n",
       "0  CRUDE OIL FALLS. Crude oil declined more than ...   2000-01   \n",
       "1  Concern over Brazilian price increases eased a...   2000-01   \n",
       "2  Merrill Lynch & Company said today that it rei...   2000-01   \n",
       "3  The Unicom Corporation and the Peco Energy Com...   2000-01   \n",
       "4  The Sysco Corporation, North America's largest...   2000-01   \n",
       "\n",
       "                                           leadstems  \n",
       "0  crude oil fall crude oil declin more than 2 pe...  \n",
       "1  concern over brazilian price increas eas after...  \n",
       "2  merril lynch compani said today that it reimbu...  \n",
       "3  the unicom corpor and the peco energi compani ...  \n",
       "4  the sysco corpor north america largest food se...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizing\n",
    "We will perform topic modeling as a means of feature reduction. Using individual stems as the features led to a high dimensional problem where none of the stems were correlated with CCI beyond the noise. Therefore, we will extract how much each document corresponds to a given topic and then try using those topics as the features to a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countVec = CountVectorizer(stop_words='english', max_df=0.8, min_df=.005, strip_accents='unicode')\n",
    "wordMatrix = countVec.fit_transform(data.leadstems)\n",
    "unigramVocab = countVec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=10)\n",
    "ldaDocs = lda.fit_transform(wordMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "topic_words = []\n",
    "\n",
    "for topic in lda.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    topic_words.append([unigramVocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topicDF = pd.DataFrame(topic_words)\n",
    "topicDF.index = ['Topic {}'.format(i) for i in range(1,11)]\n",
    "topicDF.columns = ['Stem {}'.format(i) for i in range(1,11)]\n",
    "topicDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by month\n",
    "Then, I'll group the articles by month and find the average topics by no"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AC209]",
   "language": "python",
   "name": "conda-env-AC209-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
